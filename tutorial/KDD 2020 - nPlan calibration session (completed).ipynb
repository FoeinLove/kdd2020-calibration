{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calibrate your neural network classifer:\n",
    "## Getting accurate probabilities from your neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nplan-io/kdd2020-calibration/blob/master/tutorial/KDD%202020%20-%20nPlan%20calibration%20session.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll use the CIFAR-10 image dataset, and a classifier trained on it, to explore what model confidence calibration is, how we can measure it, and what methods we can put in place to rectify poorly calibrated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10\n",
    "CIFAR-10 is a dataset containing a collection of images falling into 10 classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. It contains 60 000 low resolution (32x32) images and is often used to train ML and computer vision models. This is commonly divided into 50 000 training images and 10 000 testing images. The low resolution allows for quick testing of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[112,])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to do some preprocessing of the data. This will allow our model to achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_val = x_val.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean from each image\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_val -= x_train_mean\n",
    "x_test -= x_train_mean\n",
    "\n",
    "# translate data to categorical\n",
    "y_train_labels = y_train\n",
    "y_val_labels = y_val\n",
    "y_test_labels = y_test\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "y_val = tensorflow.keras.utils.to_categorical(y_val, 10)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The models\n",
    "The obvious next step would be training a model but as we have limited time and resources, we will be using two pre-trained models:\n",
    "- [Keras ResNet model](https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)\n",
    "- [A binary simplification of this model - that tries to discriminate between dogs and cats!](fill_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(<some layers>)\n",
    "# ...\n",
    "# model.fit(x_train, y_train)\n",
    "# ...\n",
    "# model.evaluate(x_test,y_test)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load pretrained models\n",
    "h5file = tensorflow.keras.utils.get_file('/tmp/KDD_model_1.h5',\n",
    "         'https://raw.githubusercontent.com/nplan-io/kdd2020-calibration/master/tutorial/cifar10_resnet.h5')\n",
    "multiclass_model = load_model(h5file)\n",
    "\n",
    "h5file = tensorflow.keras.utils.get_file('/tmp/KDD_model_2.h5',\n",
    "         'https://raw.githubusercontent.com/nplan-io/kdd2020-calibration/master/tutorial/cifar10_resnet_binary.h5')\n",
    "binary_model = load_model(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate the full data set to create a subset of images of cats (class 3)\n",
    "# and dogs (class 5) to use with the binary classifier\n",
    "x_val_binary = x_val[(y_val_labels==3).flatten()|(y_val_labels==5).flatten(),:,:]\n",
    "x_test_binary =  x_test[(y_test_labels==3).flatten()|(y_test_labels==5).flatten(),:,:]\n",
    "\n",
    "y_val_binary = y_val_labels[(y_val_labels==3).flatten()|(y_val_labels==5).flatten()]\n",
    "y_test_binary = y_test_labels[(y_test_labels==3).flatten()|(y_test_labels==5).flatten()]\n",
    "\n",
    "# our binary classifier will have target labels of 1 for cat and 0 for dog\n",
    "y_val_binary = [1 if target == 3 else 0 for target in y_val_binary]\n",
    "y_test_binary = [1 if target == 3 else 0 for target in y_test_binary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = binary_model.predict(x_val_binary)\n",
    "acc_score = accuracy_score(y_val_binary, y_pred_binary>0.5)\n",
    "loss_score = log_loss(y_val_binary, y_pred_binary)\n",
    "print('Binary metrics: validation accuracy is {0:.2f}, validation loss is {1:.2f}'.format(acc_score, loss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multiclass_model.predict(x_val)\n",
    "acc_score = accuracy_score(np.argmax(y_val,1),np.argmax(y_pred,1))\n",
    "loss_score = log_loss(y_val, y_pred)\n",
    "print('Multiclass metrics: validation accuracy is {0:.2f}, validation loss is {1:.2f}'.format(acc_score, loss_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding and measuring calibration - binary problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of calibration concepts were devised in binary problems, so we'll explore them using our binary model first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reliability diagrams\n",
    "Can you use `calibration_curve()` from scikit-learn to show how calibrated the model is on our data? Return two arrays, `prob_true_binary` and `prob_pred_binary`. How would you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the values of calibration curve for bin 0 vs all\n",
    "prob_true_binary, prob_pred_binary = calibration_curve(y_val_binary, y_pred_binary, n_bins=10)\n",
    "\n",
    "def plot_reliability_diagram(prob_true, prob_pred, model_name, ax=None):\n",
    "    # Plot the calibration curve for ResNet in comparison with what a perfectly calibrated model would look like\n",
    "    if ax==None:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        plt.sca(ax)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color=\"#FE4A49\", linestyle=\":\", label=\"Perfectly calibrated model\")\n",
    "    plt.plot(prob_pred, prob_true, \"s-\", label=model_name, color=\"#162B37\")\n",
    "\n",
    "    plt.ylabel(\"Fraction of positives\", fontsize=16)\n",
    "    plt.xlabel(\"Mean predicted value\", fontsize=16,)\n",
    "\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.grid(True, color=\"#B2C7D9\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_reliability_diagram(prob_true_binary, prob_pred_binary, \"ResNet (class 0 vs all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Expected calibration error\n",
    "Given the explanation of ECE, can you calculate the error for our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate ece\n",
    "def ece_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE\n",
    "    ece = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        ece = ece + (bin_sizes[m] / sum(bin_sizes)) * np.abs(prob_true[m] - prob_pred[m])\n",
    "    return ece\n",
    "\n",
    "# print the calculated ece\n",
    "n_bins_binary = len(prob_true_binary)\n",
    "pred_hist = np.histogram(a=y_pred_binary, range=(0, 1), bins=n_bins_binary)[0]     \n",
    "print(ece_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Maximum calibration error\n",
    "Given the explanation of MCE, can you calculate it for our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate mce\n",
    "def mce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE \n",
    "    mce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        mce = max(mce, np.abs(prob_true[m] - prob_pred[m]))\n",
    "    return mce\n",
    "\n",
    "#print the calculated mce\n",
    "print(mce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Root mean square calibration error\n",
    "Given the explanation, can you calculate RMSCE for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate rmsce\n",
    "def rmsce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE \n",
    "    rmsce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        rmsce = rmsce + (bin_sizes[m] / sum(bin_sizes)) * (prob_true[m] - prob_pred[m]) ** 2\n",
    "    return np.sqrt(rmsce)\n",
    "\n",
    "# print the calculated rmsce\n",
    "print(rmsce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measuring calibration in multiclass problems\n",
    "Extending the definition of these metrics, we can use them for multiclass classifiers, too. Considering each class seperately, can you show the reliability diagrams and calculate the calibration errors for the 10 class model? Can you combine them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "def ece_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    ece_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class], n_bins=10)\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        ece_bin.append(ece_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return np.mean(ece_bin)\n",
    "        \n",
    "    \n",
    "def mce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    mce_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class], n_bins=10)\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        mce_bin.append(mce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return np.mean(mce_bin)\n",
    "    \n",
    "def rmsce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    rmsce_bin = []\n",
    "    for a_class in range(y_true.shape[1]):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class], n_bins=10)\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=len(prob_true))[0]\n",
    "        rmsce_bin.append(rmsce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return np.mean(rmsce_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-training calibration methods - binary problems\n",
    "One way to mitigate a poorly calibrated model is through a post-hoc calibration method. In general, we seek a function to translate some output of our model into a calibrated probability. These come in several flavours - first we look at the binary problem, as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Non-parametric methods (Isotonic regression)\n",
    "Given the description of isotonic regression, can you fit a stepwise constant, monotonically increasing function to the bucketed softmax data? Again, scikit-learn may be useful. Plot your result on the reliability diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "iso_reg = IsotonicRegression().fit(prob_pred_binary, prob_true_binary)\n",
    "\n",
    "plot_reliability_diagram(prob_true_binary, prob_pred_binary, \"ResNet (class 0 vs all)\")\n",
    "a_grid = np.linspace(0, 1, 100)\n",
    "plt.plot(a_grid, iso_reg.predict(a_grid), label=\"Isotonic Function\")\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "y_test_pred_binary = binary_model.predict(x_test_binary)\n",
    "y_pred_corr = iso_reg.predict(y_test_pred_binary.flatten())\n",
    "\n",
    "prob_true_binary_corr, prob_pred_binary_corr = calibration_curve(y_test_binary, y_pred_corr, n_bins=10)\n",
    "\n",
    "plot_reliability_diagram(prob_true_binary_corr, prob_pred_binary_corr, \"ResNet (class 0 vs all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Platt scaling\n",
    "Now, based on the explanation, can you implement binary platt scaling for our binary classifier?\n",
    "\n",
    "[Hint - you do not necessarily need to rerun the model, and can run `scipy.special.logit()` on `y_pred_binary` to return the vector of logits]\n",
    "\n",
    "[NB - watch for zeroes!]\n",
    "\n",
    "How did it improve the calibration? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "y_pred_binary = binary_model.predict(x_val_binary)\n",
    "\n",
    "y_logits = logit(y_pred_binary)\n",
    "def scale_fun_bce(x, *args):\n",
    "    a, b = x\n",
    "    y_logit_scaled = a*y_logits + b\n",
    "    y_pred_inner = expit(y_logit_scaled)\n",
    "    bce = sum([-(y_t * np.log(y_p) + (1 - y_t) * np.log(1 - y_p)) for y_t, y_p in zip(y_val_binary[:1000], y_pred_inner) if not y_p==0])\n",
    "    return bce\n",
    "\n",
    "min_obj = minimize(scale_fun_bce,[1,0], method='Nelder-Mead',options={'xatol': 1e-8, 'disp': True})\n",
    "min_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_binary = binary_model.predict(x_test_binary)\n",
    "\n",
    "y_logits = logit(y_test_pred_binary)\n",
    "\n",
    "y_test_pred_corr = expit(min_obj.x[0]*y_logits+min_obj.x[1])\n",
    "\n",
    "prob_true_binary, prob_pred_binary = calibration_curve(y_test_binary, y_test_pred_binary, n_bins=10)\n",
    "prob_true_binary_corr, prob_pred_binary_corr = calibration_curve(y_test_binary, y_test_pred_corr, n_bins=10)\n",
    "plot_reliability_diagram(prob_true_binary, prob_pred_binary, \"ResNet (class 0 vs all)\")\n",
    "plot_reliability_diagram(prob_true_binary_corr, prob_pred_binary_corr, \"ResNet (class 0 vs all)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Calibrating multiclass models\n",
    "\n",
    "Extending to the multiclass case is not simple. Several methods have been suggested, which include treating each class as a one-vs-all binary problem, calibrating it, and then normalising the new calibrated values across classes. Another idea is to generalise Platt Scaling from a one-dimensional linear optimisation problem - we will discuss this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Example - temperature scaling\n",
    "\n",
    "A simple generalisation of Platt scaling to the multiclass case is to tune a single parameter based on the logits of the network, in order to try to optimise NLL - this is temperature scaling. First we need to access the logits of our network. We do this in Keras as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the model\n",
    "new_model = multiclass_model\n",
    "\n",
    "# get the tensor input to the final dense layer of the model\n",
    "pre_dense_out = new_model.layers[-3].output\n",
    "\n",
    "# reapply a final Dense layer - but this time with no softmax activation\n",
    "# set its weights to match the old model's dense layers\n",
    "pre_soft_out = tf.keras.layers.Dense(10, activation=None)(pre_dense_out)\n",
    "new_model = tf.keras.Model(inputs=new_model.input, outputs=pre_soft_out)\n",
    "new_model.layers[-1].set_weights(multiclass_model.layers[-2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to compile the model to predict from it\n",
    "new_model.compile(optimizer=\"Adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logit = new_model.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the logits for the validation set of our network. Following the logic of our binary platt scaling example, we now need to define the function to be optimised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def scale_fun_ce(x, *args):\n",
    "    \"\"\"Returns the NLL of the model over the validation set when scaled\n",
    "    by the t parameter\n",
    "    \"\"\"\n",
    "    t = x[0]\n",
    "    y_logit_scaled = y_logit/t\n",
    "    y_pred_inner = softmax(y_logit_scaled, axis=1)\n",
    "    return log_loss(y_val, y_pred_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "min_obj = minimize(scale_fun_ce,[1],method='Nelder-Mead',options={'xatol': 1e-13, 'disp': True})\n",
    "min_obj.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate calibration on test set\n",
    "y_logit_test = new_model.predict(x_test)\n",
    "y_test_pred = new_model.predict(x_test)\n",
    "\n",
    "# use learned scaling param to scale logits, and apply softmax\n",
    "temp_scaled = y_logit_test/min_obj.x[0]\n",
    "y_pred_test_corr = softmax(temp_scaled, axis=1)\n",
    "\n",
    "# plot pre-calibration reliability diag\n",
    "prob_true, prob_pred = calibration_curve(y_test.flatten(), y_test_pred.flatten(), n_bins=10)\n",
    "plot_reliability_diagram(prob_true, prob_pred, \"All softmax outs - uncalibrated\")\n",
    "bin_sizes = np.histogram(a=y_pred.flatten(), range=(0, 1), bins=10)[0]\n",
    "print(\"Uncalibrated RMSCE: \", rmsce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "\n",
    "# plot post-calibration reliability diag\n",
    "prob_true, prob_pred = calibration_curve(y_test.flatten(), y_pred_test_corr.flatten(), n_bins=10)\n",
    "plot_reliability_diagram(prob_true, prob_pred, \"All softmax outs - calibrated\")\n",
    "bin_sizes = np.histogram(a=y_pred.flatten(), range=(0, 1), bins=10)[0]\n",
    "print(\"Calibrated RMSCE: \", rmsce_calculation_binary(prob_true, prob_pred, bin_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementing another method\n",
    "Now it's your turn. Implement and evaluate a calibration method on the multiclass classifier. Based on the explanations you can use either matrix or vector scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use y_logit (calculated above) and complete the following function \n",
    "# to scale by a list of input params\n",
    "\n",
    "def scale_fun_mat_or_vec(x, *args):\n",
    "    \"\"\"Returns the NLL of the model over the validation set when scaled\n",
    "    by matrix or vector parameters\n",
    "    \"\"\"\n",
    "    # your x list should have as many values as your matrix and vector have\n",
    "    # use np.reshape or otherwise to retrieve an array from x\n",
    "    \n",
    "    y_logit_scaled = y_logit # do your scaling here - either matrix or dot product\n",
    "    y_pred_inner = softmax(y_logit_scaled, axis=1)\n",
    "    return log_loss(y_val, y_pred_inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scipy.optimize.minimize() to minimize your function here, as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter the code from the evaluation cell above to use your new scaling to assess the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
