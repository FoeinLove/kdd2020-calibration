{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calibrate your neural network classifer:\n",
    "## Getting accurate probabilities from your neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nplan-io/kdd2020-calibration/blob/master/tutorial/KDD%202020%20-%20nPlan%20calibration%20session.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll use the CIFAR-10 image dataset, and a classifier trained on it, to explore what model confidence calibration is, how we can measure it, and what methods we can put in place to rectify poorly calibrated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR-10\n",
    "CIFAR-10 is a dataset containing a collection of images falling into 10 classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. It contains 60 000 low resolution (32x32) images and is often used to train ML and computer vision models. This is commonly divided into 50 000 training images and 10 000 testing images. The low resolution allows for quick testing of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[112,])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to do some preprocessing of the data. This will allow our model to achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_val = x_val.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean from each image\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_val -= x_train_mean\n",
    "x_test -= x_train_mean\n",
    "\n",
    "# translate data to categorical\n",
    "y_train_labels = y_train\n",
    "y_val_labels = y_val\n",
    "y_test_labels = y_test\n",
    "\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train, 10)\n",
    "y_val = tensorflow.keras.utils.to_categorical(y_val, 10)\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The models\n",
    "The obvious next step would be training a model but as we have limited time and resources, we will be using two pre-trained models:\n",
    "- [Keras ResNet model](https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py)\n",
    "- [A binary simplification of this model - that tries to discriminate between dogs and cats!](fill_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(<some layers>)\n",
    "# ...\n",
    "# model.fit(x_train, y_train)\n",
    "# ...\n",
    "# model.evaluate(x_test,y_test)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load pretrained models\n",
    "h5file = tensorflow.keras.utils.get_file('/tmp/KDD_model_1.h5',\n",
    "         'https://raw.githubusercontent.com/nplan-io/kdd2020-calibration/master/tutorial/cifar10_resnet.h5')\n",
    "multiclass_model = load_model(h5file)\n",
    "\n",
    "h5file = tensorflow.keras.utils.get_file('/tmp/KDD_model_2.h5',\n",
    "         'https://raw.githubusercontent.com/nplan-io/kdd2020-calibration/master/tutorial/cifar10_resnet_binary.h5')\n",
    "binary_model = load_model(h5file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate the full data set to create a subset of images of cats (class 3)\n",
    "# and dogs (class 5) to use with the binary classifier\n",
    "x_val_binary = x_val[(y_val_labels==3).flatten()|(y_val_labels==5).flatten(),:,:]\n",
    "x_test_binary =  x_test[(y_test_labels==3).flatten()|(y_test_labels==5).flatten(),:,:]\n",
    "\n",
    "y_val_binary = y_val_labels[(y_val_labels==3).flatten()|(y_val_labels==5).flatten(),:,:]\n",
    "y_test_binary = y_test_labels[(y_test_labels==3).flatten()|(y_test_labels==5).flatten(),:,:]\n",
    "\n",
    "# our binary classifier will have target labels of 1 for cat and 0 for dog\n",
    "y_val_binary = [1 if target == 3 else 0 for target in y_val_binary]\n",
    "y_test_binary = [1 if target == 3 else 0 for target in y_test_binary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = binary_model.predict(x_val_binary)\n",
    "acc_score = accuracy_score(y_val_binary, y_pred_binary>0.5)\n",
    "loss_score = log_loss(y_val_binary, y_pred_binary)\n",
    "print('Binary metrics: validation accuracy is {0:.2f}, validation loss is {1:.2f}'.format(acc_score, loss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multiclass_model.predict(x_val)\n",
    "acc_score = accuracy_score(np.argmax(y_val,1),np.argmax(y_pred,1))\n",
    "loss_score = log_loss(y_val, y_pred)\n",
    "print('Multiclass metrics: validation accuracy is {0:.2f}, validation loss is {1:.2f}'.format(acc_score, loss_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding and measuring calibration - binary problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of calibration concepts were devised in binary problems, so we'll explore them using our binary model first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reliability diagrams\n",
    "Can you use `calibration_curve()` from scikit-learn to show how calibrated the model is on our data? Return two arrays, `prob_true_binary` and `prob_pred_binary`. How would you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the values of calibration curve for bin 0 vs all\n",
    "prob_true_binary, prob_pred_binary = calibration_curve(y_val_binary, y_pred_binary, n_bins=10)\n",
    "\n",
    "# Plot the calibration curve for ResNet in comparison with what a perfectly calibrated model would look like\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot([0, 1], [0, 1], color=\"#FE4A49\", linestyle=\":\", label=\"Perfectly calibrated model\")\n",
    "plt.plot(prob_pred_binary, prob_true_binary, \"s-\", label=\"ResNet (class 0 vs all)\", color=\"#162B37\")\n",
    "\n",
    "plt.ylabel(\"Fraction of positives\", fontsize=16)\n",
    "plt.xlabel(\"Mean predicted value\", fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.grid(True, color=\"#B2C7D9\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Expected calibration error\n",
    "Given the explanation of ECE, can you calculate the error for our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate ece\n",
    "def ece_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE\n",
    "    ece = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        ece = ece + (bin_sizes[m] / sum(bin_sizes)) * np.abs(prob_true[m] - prob_pred[m])\n",
    "    return ece\n",
    "\n",
    "# print the calculated ece\n",
    "n_bins_binary = len(prob_true_binary)\n",
    "pred_hist = np.histogram(a=y_pred_binary, range=(0, 1), bins=n_bins_binary)[0]     \n",
    "print(ece_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Maximum calibration error\n",
    "Given the explanation of MCE, can you calculate it for our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate mce\n",
    "def mce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE \n",
    "    mce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        mce = max(mce, np.abs(prob_true[m] - prob_pred[m]))\n",
    "    return mce\n",
    "\n",
    "#print the calculated mce\n",
    "print(mce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Root mean square calibration error\n",
    "Given the explanation, can you calculate RMSCE for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete this function to calculate rmsce\n",
    "def rmsce_calculation_binary(prob_true, prob_pred, bin_sizes):\n",
    "    ### YOUR CODE HERE \n",
    "    rmsce = 0\n",
    "    for m in np.arange(len(bin_sizes)):\n",
    "        rmsce = rmsce + (bin_sizes[m] / sum(bin_sizes)) * (prob_true[m] - prob_pred[m]) ** 2\n",
    "    return np.sqrt(rmsce)\n",
    "\n",
    "# print the calculated rmsce\n",
    "print(rmsce_calculation_binary(prob_true_binary, prob_pred_binary, pred_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Measuring calibration in multiclass problems\n",
    "Extending the definition of these metrics, we can use them for multiclass classifiers, too. Considering each class seperately, can you show the reliability diagrams and calculate the calibration errors for the 10 class model? Can you combine them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "def ece_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    ece_bin = []\n",
    "    for a_class in range(y_true):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class])\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=n_bins_0)[0]\n",
    "        ece_bin.append(ece_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return mean(ece_bin)\n",
    "        \n",
    "    \n",
    "def mce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    mce_bin = []\n",
    "    for a_class in range(y_true):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class])\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=n_bins_0)[0]\n",
    "        mce_bin.append(mce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return mean(mce_bin)\n",
    "    \n",
    "def rmsce_calculation_multiclass(y_true, y_pred):\n",
    "    ### use calibration_curve and your binary function to complete this function\n",
    "    rmsce_bin = []\n",
    "    for a_class in range(y_true):\n",
    "        prob_true, prob_pred = calibration_curve(y_true[a_class], y_pred[a_class])\n",
    "        bin_sizes = np.histogram(a=y_pred[a_class], range=(0, 1), bins=n_bins_0)[0]\n",
    "        rmsce_bin.append(rmsce_calculation_binary(prob_true, prob_pred, bin_sizes))\n",
    "    ## here we have a choice - do we wish to weight our metric depending on the number\n",
    "    ## of positive examples in each class, or take an unweighted mean\n",
    "    \n",
    "    # return sum(ece_bin*class_weights)/n_classes\n",
    "    return mean(rmsce_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-training calibration methods - binary problems\n",
    "One way to mitigate a poorly calibrated model is through a post-hoc calibration method. In general, we seek a function to translate some output of our model into a calibrated probability. These come in several flavours - first we look at the binary problem, as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Non-parametric methods (Isotonic regression)\n",
    "Given the description of isotonic regression, can you fit a stepwise constant, monotonically increasing function to the bucketed softmax data? Again, scikit-learn may be useful. Plot your result on the reliability diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "iso_reg = IsotonicRegression().fit(prob_pred_binary, prob_true_binary)\n",
    "\n",
    "plt.plot(prob_pred_binary, prob_true_binary, \"s-\", label=\"ResNet (class 0 vs all)\", color=\"#162B37\")\n",
    "a_grid = np.linspace(0, 1, 100)\n",
    "plt.plot(a_grid, iso_reg.predict(a_grid))\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.show()\n",
    "\n",
    "#y_pred = model.predict(x_test)\n",
    "y_pred_corr = iso_reg.predict(y_pred_0)\n",
    "\n",
    "prob_true_0_corr, prob_pred_0_corr = calibration_curve(y_test_0, y_pred_corr, n_bins=10)\n",
    "\n",
    "plt.plot(prob_pred_0_corr, prob_true_0_corr, \"s-\", label=\"ResNet (class 0 vs all)\", color=\"#162B37\")\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Platt scaling\n",
    "Now, based on the explanation, can you implement binary platt scaling for our binary classifier?\n",
    "\n",
    "[Hint - you do not necessarily need to rerun the model, and can run `scipy.special.logit()` on `y_pred_binary` to return the vector of logits]\n",
    "\n",
    "How did it improve the calibration? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def scale_fun_bce(x, *args):\n",
    "    a, b = x\n",
    "    y_logit_scaled = a*y_logits + b\n",
    "    y_pred = expit(y_logit_scaled)\n",
    "    bce = sum([-(y_t * np.log(y_p) + (1 - y_t) * np.log(1 - y_p)) for y_t, y_p in zip(y_test_small, y_pred)])\n",
    "    return bce\n",
    "\n",
    "min_obj = minimize(scale_fun_bce,[1,0], method='Nelder-Mead',options={'xatol': 1e-15, 'disp': True})\n",
    "\n",
    "platt_params = min_obj.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Calibrating multiclass models\n",
    "\n",
    "Extending to the multiclass case is not simple. Several methods have been suggested, which include treating each class as a one-vs-all binary problem, calibrating it, and then normalising the new calibrated values across classes. Another idea is to generalise Platt Scaling from a one-dimensional linear optimisation problem - we will discuss this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Keras example -  temperature scaling\n",
    "A very simple generalisation of Platt scaling is to tune a single parameter based on the logits of the network, in order to try to optimise NLL - this is temperature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to define a Keras layer that does the transformation we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class TemperatureLayer(tensorflow.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A keras layer that divides the input tensor elementwise by a single trainable parameter\n",
    "    temp_weight\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialises layer with weight param\n",
    "        \"\"\"\n",
    "        self.temp_weight = None\n",
    "        super(TemperatureLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Defines the trainable parameters of the model\n",
    "        \"\"\"\n",
    "        self.temp_weight = self.add_weight(\n",
    "            name=\"temp\",\n",
    "            shape=(1,),\n",
    "            initializer='one'\n",
    "        )\n",
    "        super(TemperatureLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, in_tensor):\n",
    "        \"\"\"\n",
    "        Divide input tensor by constant t\n",
    "        \"\"\"\n",
    "        return tf.divide(in_tensor, self.temp_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take our trained model and insert the temperature layer prior to the softmax output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model\n",
    "# remove the original softmax\n",
    "new_model.layers.pop()\n",
    "# pipe the intermediate output through our new layer\n",
    "output = new_model.layers[-1].output\n",
    "output = TemperatureLayer(name=\"calibration\", trainable=False)(output)\n",
    "# add the softmax again\n",
    "output = tf.keras.layers.Activation('softmax', name='new_softmax')(output)\n",
    "# rebuild the model\n",
    "new_model = tf.keras.Model(inputs=new_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see when we  that we've inserted the layer into the model\n",
    "new_model.layers[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fix our pretrained weights, allow the calibration weights to be trained, and recompile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in new_model.layers:\n",
    "    if \"calibration\" in layer.name:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "new_model.compile(optimizer=\"Adam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model using Keras `model.fit` - and only our calibration layer weights will be trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the reliability diagrams and use your metric calculators to calculate the calibration metrics for this new model\n",
    "# have they improved?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementing a scaling layer\n",
    "Now it's your turn. Implement and evaluate a calibration method on the multiclass classifier. Based on the explanations you can use either matrix or vector platt layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this class to encapsulate the vector or matrix scaling logic as explained in the presentation\n",
    "\n",
    "# (hint - keras.layers.Dense may help for matrix platt!)\n",
    "\n",
    "### This is an implementation of the vector scaling layer:\n",
    "class MyCalLayer(tf.keras.layers.Layer):\n",
    "    \"\"\" A template keras layer \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\" Initialises layer with weight params \"\"\"\n",
    "        ###\n",
    "        # initialise weight variables here\n",
    "        ###\n",
    "        \n",
    "        # we need two vectors:\n",
    "        self.weight_vector = None\n",
    "        self.bias_vector = None\n",
    "        super(MyCalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\" Defines the trainable parameters of the model \"\"\"\n",
    "        ###\n",
    "        # add keras weight objs of the correct shape to\n",
    "        # the respective varibles here\n",
    "        ###\n",
    "        \n",
    "        # use the add_weight method to add a keras weight - \n",
    "        # they should be tensors of the same size as the input\n",
    "        self.weight_vector = self.add_weight(\n",
    "            name=\"platt_weight\",\n",
    "            shape=(input_shape[1],),\n",
    "            initializer='one'\n",
    "        )\n",
    "        self.bias_vector = self.add_weight(\n",
    "            name=\"platt_bias\",\n",
    "            shape=(input_shape[1],),\n",
    "            initializer='zero'\n",
    "        )\n",
    "        super(MyCalLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, in_tensor):\n",
    "        \"\"\" undertakes layer transformation \"\"\"\n",
    "        ###\n",
    "        # use the relevant tf functions to manipulate \n",
    "        # tensors as required, and return result\n",
    "        ###\n",
    "        \n",
    "        # we want to carry out a dot product and an addition:\n",
    "        # this is how we do so in tf\n",
    "        return tf.multiply(in_tensor, self.weight_vector) + self.bias_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the steps of the temperature scaling example above to calibrate and evaluate your model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
